# Neural Networks (Feed Forward, RNN) from Scratch

The main objective of this project is to explore how to implement deep learning structures from scratch by using their underlying mathematical principles, rather than relying on existing machine learning libraries and frameworks. Examining how neural networks work internally has been extremely valuable towards building a personal understanding of the fundamental NNet architectures that modern AI runs on.

This repo currently contains my implementation of 
- deep feed-forward neural networks
- **recurrent neural networks** (RNN)
using self-derived back propagation through time, and general matrix differentiation.
